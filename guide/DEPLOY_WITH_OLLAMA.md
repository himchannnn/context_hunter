# Local AI Deployment Guide (Ollama + RTX A5000)

ì´ ê°€ì´ë“œëŠ” **RTX A5000 (24GB)** GPUê°€ ì¥ì°©ëœ ì„œë²„ì—ì„œ **Ollama**ë¥¼ ì‚¬ìš©í•˜ì—¬ AIë¥¼ ë¡œì»¬ë¡œ êµ¬ë™í•˜ê³ , Context Hunter ì„œë¹„ìŠ¤ì™€ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## 1. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ì¤€ë¹„

ì„œë²„(Linux)ì— Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  Llama 3.1 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

### 1.1 Ollama ì„¤ì¹˜
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 1.2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Llama 3.1 8b)
RTX A5000ì€ 24GB VRAMì„ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ, 8b ëª¨ë¸ì€ ë§¤ìš° ì—¬ìœ ë¡­ê²Œ ëŒì•„ê°‘ë‹ˆë‹¤. (ì•½ 6GB VRAM ì†Œìš”)

```bash
ollama pull llama3.1
```

### 1.3 Ollama ì‹¤í–‰ í™•ì¸
Ollamaê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. ê¸°ë³¸ í¬íŠ¸ëŠ” **11434**ì…ë‹ˆë‹¤.

```bash
curl http://localhost:11434/api/tags
# {"models":[{"name":"llama3.1:latest", ...}]} ì™€ ê°™ì€ ì‘ë‹µì´ ì˜¤ë©´ ì„±ê³µ
```

---

## 2. Backend ì—°ê²° ì„¤ì •

Context Hunterì˜ ë°±ì—”ë“œê°€ ë¡œì»¬ Ollamaë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.

### 2.1 í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env)
`backend/.env` íŒŒì¼ (ë˜ëŠ” Dockerì˜ `.env`)ì„ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•©ë‹ˆë‹¤.

```ini
# AI Configuration for Local Ollama
AI_API_KEY=ollama  # OllamaëŠ” í‚¤ê°€ í•„ìš” ì—†ì§€ë§Œ, í´ë¼ì´ì–¸íŠ¸ í˜¸í™˜ì„±ì„ ìœ„í•´ ì„ì˜ì˜ ê°’ ì…ë ¥
AI_BASE_URL=http://host.docker.internal:11434/v1  # Docker ì‚¬ìš© ì‹œ
# ë˜ëŠ”
# AI_BASE_URL=http://localhost:11434/v1  # ë¡œì»¬ ì§ì ‘ ì‹¤í–‰ ì‹œ

AI_MODEL_NAME=llama3.1
```

> **ì£¼ì˜ (Docker ì‚¬ìš© ì‹œ):**
> Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í˜¸ìŠ¤íŠ¸ì˜ Ollamaì— ì ‘ê·¼í•˜ë ¤ë©´ `host.docker.internal`ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
> Linux Dockerì—ì„œëŠ” `docker-compose.yml`ì— `extra_hosts` ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.2 Docker Compose ì„¤ì • (Linux ì„œë²„ìš©)
`docker-compose.yml` íŒŒì¼ì„ ì—´ì–´ `backend` ì„œë¹„ìŠ¤ì— ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì—¬ í˜¸ìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ì„ í—ˆìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì…ë‹ˆë‹¤.

**ë°©ë²• A: host.docker.internal ë§¤í•‘ (ì¶”ì²œ)**
```yaml
services:
  backend:
    # ... ê¸°ì¡´ ì„¤ì • ...
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

**ë°©ë²• B: Host Network ëª¨ë“œ (ëŒ€ì•ˆ)**
```yaml
services:
  backend:
    network_mode: "host"
    # ... í¬íŠ¸ ë§¤í•‘ ì œê±° í•„ìš” ...
```

---

## 3. ì „ì²´ ë°°í¬ ìˆœì„œ ìš”ì•½

1.  **Ollama ì„¤ì¹˜ & ëª¨ë¸ Pull**: `ollama pull llama3.1`
2.  **í”„ë¡œì íŠ¸ í´ë¡ **: `git clone ...`
3.  **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**: `.env` íŒŒì¼ì— `AI_BASE_URL=http://host.docker.internal:11434/v1` ì„¤ì •
4.  **Docker Compose ìˆ˜ì •**: `docker-compose.yml`ì— `extra_hosts` ì¶”ê°€
5.  **ì„œë¹„ìŠ¤ ì‹¤í–‰**: `docker-compose up -d --build`

ì´ì œ ì™¸ë¶€ API ë¹„ìš© ì—†ì´, ê°•ë ¥í•œ RTX A5000 GPUë¥¼ í™œìš©í•˜ì—¬ ë¬´ì œí•œìœ¼ë¡œ ë¬¸ì œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
